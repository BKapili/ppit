---
title: "PPIT tutorial (v.1.2.0)"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{PPIT tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

-----
Phylogenetic Placement for Inferring Taxonomy (PPIT) is an R package for inferring source organism identity of _nifH_ sequences. For additional details, see Kapili & Dekas, in prep.
-----

## Getting started

To install `ppit` directly from RStudio, use the `devtools` package:

``` r
# install.packages("devtools")
devtools::install_github("BKapili/ppit")
```
Now, let's load the `ppit` package to get started.

```{r message = FALSE}
library(ppit)
library(phyloseq)
library(Biostrings)
library(ape)
library(ggplot2)
```

In this tutorial, we'll work with:

* _nifH_ amplicon data generated from 2x250 paired-end Illumina MiSeq sequencing
* SEPP (v.4.3.5) output alignment of reference and query _nifH_ sequences
* SEPP output tree of reference and query _nifH_ sequences

These data have already been processed as outlined in Kapili & Dekas, in prep. and are included in the package. For help with the SEPP placement, see the bonus section at the end of this vignette or the [SEPP tutorial](https://github.com/smirarab/sepp/blob/master/tutorial/sepp-tutorial.md).


## Optimize operational phylogenetic neighborhood

Here, we'll optimize the operational phylogenetic neighborhood. This neighborhood defines the maximum distance between a query and reference sequence we'll consider when making host inferences. The actual parameter that we're optimizing is the maximum sum of branch lengths (\emph{i}.\emph{e}., patristic distance) we'll consider between a query and reference sequence. Here, we're defining the optimal value as that which maximizes the number of inferences returned at the phylum rank. 

Note that, while similar in concept, the phylogenetic neighborhood is different than the rank cutoffs. The primary goal of setting an upper bound to the sum of branch lengths allowed between a query and reference sequence is to reduce the number of query sequences mistakenly flagged for potential horizontal gene transfer. On their own, the patristic distance rank cutoffs have the tendency to overexplore tree space and mischaracterize vertical inheritance as horizontal inheritance (see Kapili & Dekas, in prep.). The value of the operational phylogenetic neighborhood depends on the query set and can be lower than the phylum/class/order/family cutoffs, but cannot be lower than the genus cutoff. This requires that, at minimum, we will use all reference sequences within the genus cutoff during inferencing. Typically, this step increases `ppit`'s ability to draw inferences.

To do this, we run the function `tree.partition`, which requires 6 arguments:

* `type`: specifies whether the query sequences are `partial` (e.g., amplicons) or `full` sequences (e.g., from metagenome-assembled genomes). This argument affects how we calculate pairwise percent identity (see `?tree.partition` for more details)

* `query`: vector of the query names for which we want to infer host identity

* `tree`: reference tree with query sequences placed into it

* `alignment`: reference nucleotide alignment with inserted query sequences

* `taxonomy`: data frame containing the taxonomic and supplementary information for the reference sequences

* `cutoffs`: data frame specifying patristic distance and pairwise percent identity taxonomic rank cutoffs

To save time, the optimal phylogenetic neighborhood was previously calculated (= 0.33). Since it is lower than the genus patristic distance rank cutoff (= 0.332), we will set it to that value.

```{r}
type <- "partial"

ps <- ppit::nifH_example_ps

query <- names(refseq(ps))

tree <- ppit::nifH_example_SEPP_tree

alignment <- ppit::nifH_example_SEPP_alignment

taxonomy <- ppit::nifH_reference_taxonomy_v2

cutoffs <- ppit::nifH_cutoffs_v2

#opt_thresh <- tree.partition(type, query, tree, alignment, taxonomy, cutoffs)
opt_thresh <- cutoffs[1,5]
```

## Infer host identity

Now we're ready to infer host identities for our sequences! All we have to do is run the function `ppit`, which requires the same arguments as `tree.partition` in addition to the optimal neighborhood value we just calculated.

```{r results = 'hide'}
df_inferences <- ppit(type, query, tree, alignment, taxonomy, opt_thresh, cutoffs)
```

Here, we'll run `ppit` again on the subset of sequences that didn't have any references within the optimal phylogenetic neighborhood. We'll use the phylum cutoff as the neighborhood. Then, we'll add the inferences as the taxonomy table to our phyloseq object.

```{r results = 'hide'}
requery <- rownames(df_inferences)[which(df_inferences$Pat_dist_fail == "X")]

df_requery <- ppit(type = type, query = requery, tree = tree, alignment = alignment, taxonomy = taxonomy, opt_thresh = cutoffs[1,1], cutoffs = cutoffs)

df_inferences[match(rownames(df_requery), rownames(df_inferences)),] <- df_requery

tax_table <- as.matrix(df_inferences[match("Domain", colnames(df_inferences)):match("Genus", colnames(df_inferences))])

tax_table[which(df_inferences$Suspected_homolog == "X"),] <- "SUSP_HOMOLOG"
tax_table[which(df_inferences$Percent_id_fail == "X"),] <- "Unidentified"
tax_table[which(df_inferences$Pat_dist_fail == "X"),] <- "Unidentified"
tax_table[which(df_inferences$Potential_HGT == "X"),] <- "Unidentified"

tax_table(ps) <- tax_table
```

### Saving output (optional)
If we want to save our results, we can run this chunk to write the ASV table and PPIT results data frame to tab-delimited files and the accompanying sequences to a text file.

```{r}
#write.table(t(otu_table(ps)), file = "FILL_PATH", col.names = TRUE, row.names = TRUE, quote = FALSE, sep = "\t")

#write.table(df_inferences, file = "FILL_PATH", col.names = TRUE, row.names = TRUE, quote = FALSE, sep = "\t")

#write.table(as.data.frame(refseq(ps)), file = "FILL_PATH", col.names = TRUE, row.names = TRUE, quote = FALSE, sep = "\t")
```

## Brief analysis
And that's all! Let's quickly inspect our results. Here, we'll make a barplot where we plot the relative abundance of only the _nifH_ sequences from each class from _Proteobacteria_ and from each phylum for all other groups.

```{r}
# Subset to just the sample we are interested in.
keep_samps <- c("nifH-mehta-3500-sed-0cm25cm-C2-rep1")
keep_samp_names <- grep(paste(keep_samps, collapse = "|"), sample_names(ps), value = TRUE)
ps_nifH <- subset_samples(ps, sample_names(ps) %in% keep_samp_names)

# Prune any ASVs that are not present in these samples or were identified as suspected homologs.
ps_nifH <- prune_taxa(taxa_sums(ps_nifH) > 0, ps_nifH)
ps_nifH <- subset_taxa(ps_nifH, Phylum !="SUSP_HOMOLOG")

# Convert reads to relative abundances
ps_nifH <- transform_sample_counts(ps_nifH, function(x) x/sum(x))

# Break Proteobacteria into class
tax_table(ps_nifH)[which(tax_table(ps_nifH)[,2] == "Proteobacteria"),2] <- tax_table(ps_nifH)[which(tax_table(ps_nifH)[,2] == "Proteobacteria"),3]

df_ps_nifH <- psmelt(ps_nifH)

# Create a data frame where the abundances are plotted using group abundances and not individual ASVs. Also records the number of ASVs found in each phylum (or class for Proteobacteria) and their avg %id +/- standard deviation.
df_plot <- data.frame(Phylum = unique(df_ps_nifH$Phylum))

# Record total relative abundance for the group.
abundance.sum <- function(tax, df) {
  x <- sum(df[grep(tax, df[,12]), 3])
  
  return(x*100)
}

df_plot$Relative_abundance <- lapply(df_plot$Phylum, abundance.sum, df = df_ps_nifH)

# Record the number of ASVs in the group.
no.asvs <- function(tax, df) {
  x <- length(grep(tax, df$Phylum))
  
  return(x)
}

df_plot$No_ASVs <- lapply(df_plot$Phylum, no.asvs, df = df_ps_nifH)

# Record the average percent identity between the queries and closest references.
avg.id <- function(tax, df_phylo, df_ppit) {
  x <- df_phylo[grep(tax, df_phylo[,12]), 1]
  
  y <- as.numeric(df_ppit[match(x, rownames(df_inferences)), 5])*100
  
  return(paste(mean(y), sd(y), sep = " +/- "))
}

df_plot$Avg_id <- lapply(df_plot$Phylum, avg.id, df_phylo  = df_ps_nifH, df_ppit = df_inferences)

# Reorder the groups to match the order in which we want them to appear in the barchart.
df_plot$Phylum <- factor(df_plot$Phylum, levels = c("Planctomycetes", "Lentisphaerae", "Gammaproteobacteria", "Kiritimatiellaeota", "Deltaproteobacteria", "Unidentified"))

# Add the name of the sample we're analyzing
df_plot$Sample <- "nifH-mehta-3500-sed-0cm25cm-C2-rep1"

ggplot(data = df_plot,
       mapping = aes_string(x = "Sample", y = "Relative_abundance")) +
  
  geom_bar(aes(fill = Phylum), color = "black", stat = "identity") +
  
  scale_fill_manual(values = c("Kiritimatiellaeota" = "#eb4d55",
                               "Lentisphaerae" = "#f6e1e1",
                               "Planctomycetes" = "black",
                               "Deltaproteobacteria" = "#333366",
                               "Gammaproteobacteria" = "#ff9d76",
                               "Unidentified" = "grey60")) +
  theme_bw() +
  theme(axis.text.x = element_blank(),
        panel.grid = element_blank())

```

Plot up histogram of percent identity for select Desulfuromonadales ASVs. 

```{r}
# Read in data frame containing the ASV names of those clustering with G. electrodiphilus, D. kysingii, and the Desulfuromonas metagenomes.
names <- ppit::desulfuro_asvs

# For each column name, corresponding to one of the reference sequences in the Desulfuromonadales, compute the percent identity with each ASV.
for(x in 1:ncol(names)) {
  names[,x] <- unlist(lapply(rownames(names),
       ppit::percent.identity, type = "partial",
       group = colnames(names)[x],
       alignment_as_matrix = as.matrix(ppit::nifH_example_SEPP_alignment)))
}

# Create a new column where, for each ASV, we record the highest percent identity with respect to the Desulfuromonas sequences.
for(x in 1:nrow(names)) {
   names[x,7] <- max(names[x,grep("Desulfuromonas", colnames(names))])
}
colnames(names)[7] <- "Desulfuromonas_max"

# Plot up histogram of percent identities for only those ASVs.
p_geopsychro <- ggplot() +
  geom_histogram(data = names, 
                 aes(x = names[,5]*100),
                 binwidth = 1,
                 fill = "gray20") +
  
  geom_vline(xintercept = nifH_cutoffs_v2[2,3]*100) +
  geom_vline(xintercept = nifH_cutoffs_v2[2,4]*100) +
  geom_vline(xintercept = nifH_cutoffs_v2[2,5]*100) +
  
  theme_bw() +
  
  coord_cartesian(xlim = c(70, 92), ylim = c(0, 12)) +
  scale_y_continuous(expand = c(0,0)) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank()) +
  
  labs(x = "Pairwise identity (%)",
       y = "Number of ASVs")

p_desulfuromusa <- ggplot() +
  geom_histogram(data = names, 
                 aes(x = names[,6]*100),
                 binwidth = 1,
                 fill = "gray20") +
  
  geom_vline(xintercept = nifH_cutoffs_v2[2,3]*100) +
  geom_vline(xintercept = nifH_cutoffs_v2[2,4]*100) +
  geom_vline(xintercept = nifH_cutoffs_v2[2,5]*100) +
  
  theme_bw() +
  
  coord_cartesian(xlim = c(70, 92), ylim = c(0, 12)) +
  scale_y_continuous(expand = c(0,0)) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank()) +
  
    labs(x = "Pairwise identity (%)",
         y = "Number of ASVs")

p_desulfuromonas <- ggplot() +
  geom_histogram(data = names, 
                 aes(x = names[,7]*100),
                 binwidth = 1,
                 fill = "gray20") +
  
  geom_vline(xintercept = nifH_cutoffs_v2[2,3]*100) +
  geom_vline(xintercept = nifH_cutoffs_v2[2,4]*100) +
  geom_vline(xintercept = nifH_cutoffs_v2[2,5]*100) +
  
  theme_bw() +
  
  coord_cartesian(xlim = c(70, 92), ylim = c(0,12)) +
  scale_y_continuous(expand = c(0,0)) +
  
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank()) +
  
  labs(x = "Pairwise identity (%)",
       y = "Number of ASVs")

p_geopsychro
p_desulfuromusa
p_desulfuromonas
```


## Bonus: Using `dada2` to infer nifH ASVs. Code adapted from the [dada2 tutorial](https://benjjneb.github.io/dada2/tutorial.html).

Set the paths to each primer set.
```{r}
#path_mehta <- "FILL_PATH" # Replace with location of downloaded sequences
#fns_mehta <- list.files(path_mehta)
#fastqs_mehta <- fns_mehta[grepl(".fastq$", fns_mehta)]
#fastqs_mehta <- sort(fastqs_mehta) # Sort ensures forward/reverse reads are in same order

#fnFs_mehta <- fastqs_mehta[grepl("_R1", fastqs_mehta)] # Just the forward read files
#fnRs_mehta <- fastqs_mehta[grepl("_R2", fastqs_mehta)] # Just the reverse read files
```

Get sample names, assuming files named as: SAMPLENAME_XXX.fastq.
```{r}
#sample.names_mehta <- sapply(strsplit(fnFs_mehta, "_"), `[`, 1)
```

Specify the full path to the fnFs and fnRs.
```{r}
#fnFs_mehta <- file.path(path_mehta, fnFs_mehta)
#fnRs_mehta <- file.path(path_mehta, fnRs_mehta)

#plotQualityProfile(fnFs_mehta[[10]])
#plotQualityProfile(fnRs_mehta[[10]])
```

Make directory and filenames for the filtered fastqs.
```{r}
#filt_path_mehta <- file.path(path_mehta, "filtered")
#if(!file_test("-d", filt_path_mehta)) dir.create(filt_path_mehta)

#filtFs_mehta <- file.path(filt_path_mehta, paste0(sample.names_mehta, "_F_filt.fastq"))
#filtRs_mehta <- file.path(filt_path_mehta, paste0(sample.names_mehta, "_R_filt.fastq"))

```

Filter previously trimmed fastq files.
```{r}
# Filter those from mehta
#for(i in seq_along(fnFs_mehta)) {
#  fastqPairedFilter(c(fnFs_mehta[i], fnRs_mehta[i]), c(filtFs_mehta[i], filtRs_mehta[i]),
#                    truncLen=c(220,220), 
#                    maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
#                    compress=F, verbose=TRUE)
#}
```

Make sure vectors only point to written files, in event sample ends up with 0 sequences after filtering.
```{r}
#fns_filt_mehta <- list.files(filt_path_mehta)
#fastqs_filt_mehta <- fns_filt_mehta[grepl(".fastq$", fns_filt_mehta)]
#fastqs_filt_mehta <- sort(fastqs_filt_mehta)
#filtFs_mehta <- fastqs_filt_mehta[grepl("_F", fastqs_filt_mehta)] 
#filtRs_mehta <- fastqs_filt_mehta[grepl("_R", fastqs_filt_mehta)]

#sample.names.filt_mehta <- sapply(strsplit(filtFs_mehta, "_"), `[`, 1)
```

Dereplicate sequence files.
```{r}
#setwd(filt_path_mehta)
#derepFs_mehta <- derepFastq(filtFs_mehta, verbose=TRUE)
#derepRs_mehta <- derepFastq(filtRs_mehta, verbose=TRUE)
```

Name the derep-class objects by the sample names.
```{r}
#names(derepFs_mehta) <- sample.names.filt_mehta
#names(derepRs_mehta) <- sample.names.filt_mehta
```

Determine error rates.
```{r}
#dadaFs.lrn_mehta <- dada(derepFs_mehta, err=NULL, selfConsist = TRUE, multithread=TRUE)
#errF_mehta <- dadaFs.lrn_mehta[[1]]$err_out

#dadaRs.lrn_mehta <- dada(derepRs_mehta, err=NULL, selfConsist = TRUE, multithread=TRUE)
#errR_mehta <- dadaRs.lrn_mehta[[1]]$err_out

#plotErrors(dadaFs.lrn_mehta[[1]], nominalQ=TRUE)
```

Infer the sequence variants.
```{r}
#dadaFs_mehta <- dada(derepFs_mehta, err=errF_mehta, multithread=TRUE, pool=TRUE)
#dadaRs_mehta <- dada(derepRs_mehta, err=errR_mehta, multithread=TRUE, pool=TRUE)
```

Merge paired-end reads.
```{r}
#mergers_mehta <- mergePairs(dadaFs_mehta, derepFs_mehta, dadaRs_mehta, derepRs_mehta, verbose=TRUE)
```

Make an 'OTU' (ASV) table.
```{r}
#seqtab_mehta <- makeSequenceTable(mergers_mehta)
#dim(seqtab_mehta)
```

Inspect distribution of sequence lengths.
```{r}
#table(nchar(getSequences(seqtab_mehta)))
```

Filter sequences to retain just those between 320-367 bp.
```{r}
#seqtab2_mehta <- seqtab_mehta[,nchar(colnames(seqtab_mehta)) %in% seq(320,367)]
```

Remove chimeric sequences.
```{r}
#seqtab.nochim_mehta <- removeBimeraDenovo(seqtab2_mehta, verbose=TRUE)
#dim(seqtab.nochim_mehta) #how many sequences passed 
#sum(seqtab.nochim_mehta)/sum(seqtab2_mehta) #percentage of sequences not chimeras
```

Create dataframe with sample information.
```{r}
#otu.seqs_mehta <- colnames(seqtab.nochim_mehta)
#otu.names_mehta <- paste("ASV",1:length(otu.seqs_mehta),sep=".")
#otu.df_mehta <- data.frame(otu.names_mehta, otu.seqs_mehta)
#seqtab.otu_mehta <- seqtab.nochim_mehta
#colnames(seqtab.otu_mehta) <- otu.df_mehta$otu.names_mehta

#samples.out <- rownames(seqtab.otu_mehta)
#project <- sapply(strsplit(samples.out, "-"), `[`, 1)
#primer <- sapply(strsplit(samples.out, "-"), `[`, 2)
#depth <- sapply(strsplit(samples.out, "-"), `[`, 3)
#environ <- sapply(strsplit(samples.out, "-"), `[`, 4)
#site_depth <- sapply(strsplit(samples.out, "-"), `[`, 5)
#core <- sapply(strsplit(samples.out, "-"), `[`, 6)
#rep <- sapply(strsplit(samples.out, "-"), `[`, 7)

#samdf_mehta <- data.frame(Project=project,Primer=primer,Depth=depth,
#                      Environ=environ,Site_depth=site_depth, Core=core,Rep=rep)
#rownames(samdf_mehta) <- samples.out

#seqs_mehta <- DNAStringSet(otu.df_mehta$otu.seqs_mehta)
#names(seqs_mehta) <- otu.df_mehta$otu.names_mehta
```

Make phyloseq object that incorporates sample information data frame (samdf), OTU table (seqtab.otu), taxonomy information (taxa), and sequence data (seqs). Then, prune singletons.
```{r}
#psMehta <- phyloseq(otu_table(seqtab.otu_mehta, taxa_are_rows=FALSE), sample_data(samdf_mehta), refseq(seqs_mehta))
#psMehta <- prune_taxa(taxa_sums(psMehta) > 1, psMehta)
```

```{r}
# Write the ASVs to a fasta file
#writeXStringSet(refseq(psMehta), filepath = 'INSERT_PATH',format='fasta')

# Write the reference alignment to a fasta file
#writeXStringSet(DNAStringSet(ppit::nifH_reference_alignment_v2), filepath='INSERT_PATH')

# Write the reference tree to a Newick file
#write.tree(ppit::nifH_reference_tree_v2, file = 'INSERT_PATH')

# Write out the RAxML info file
#write.table(ppit::nifH_reference_RAxML_info_v2, file = 'INSERT_PATH', quote = FALSE, col.names = FALSE, row.names = FALSE)
```

At this step, run `SEPP` using the written files. Check the output alignment, record which ASVs did not align to the target region, and remove them from the phyloseq object.
```{r}
#nifH_mehta_ASV_to_remove <- c("ASV.1003",
#                              "ASV.1004",
#                              "ASV.1064",
#                              "ASV.1090",
#                              "ASV.1157",
#                              "ASV.1213",
#                              "ASV.1229",
#                              "ASV.1230",
#                              "ASV.1255",
#                              "ASV.424",
#                              "ASV.588",
#                              "ASV.626",
#                              "ASV.784",
#                              "ASV.959")

#psMehta <- prune_taxa(taxa_names(psMehta) %in% nifH_mehta_ASV_to_remove == FALSE, psMehta) # Remove poorly aligned ASVs
```

## Bonus: Use SEPP to insert nifH ASVs and remove poorly aligned sequences.

SEPP for placement
```{r SEPP placement}
# Insert nifH ASVs into the reference alignment and tree. Before running, make sure there are no "/" characters in any of the sequence names in the files. Also, make sure there are no "--" in any of the sequence fragments, because for some reason SEPP doesn't run for sequences with that in the name

# -t is the input tree (in this case the reference nifH tree; ppit::nifH_reference_tree_v2)
# -r is the RAxML info file
# -a is the backbone alignment (in this case the reference nifH alignment; ppit::nifH_reference_alignment_v2)
# -f is the fasta file containing sequence fragments (in this case nifH ASVs)

#run_sepp.py -t PATH_TO_REFERENCE_TREE -r PATH_TO_RAXML_INFO_FILE -a PATH_TO_REFERENCE_ALIGNMENT -f PATH_TO_FASTA_FILE

#"INSERT PATH TO guppy PACKAGE" tog --xml output_placement.json
```
